{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tLrkoeHQ_mDW"
   },
   "source": [
    "# Project description \n",
    "The following blog post is chosen as framework for this project. Listed below is also a few possible tweaks to improve its performance:\n",
    "\n",
    "*   Other pre-trained model: ResNET, Inception...\n",
    "*   Added approaches from other papers\n",
    "\n",
    "[How to Develop a Deep Learning Photo Caption Generator from Scratch](https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0HY8pByX_GW_"
   },
   "source": [
    "# **Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YRcp1O84_Sfz",
    "outputId": "e8b85473-3125-4795-c551-c66e90b849e9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "from os import listdir\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "\n",
    "# Image processing, pre-trained VGG16 & InceptionResNetV2\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.vgg16 import preprocess_input as preprocess_VGG16\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input as preprocess_InceptionResNet\n",
    "\n",
    "# Text processing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.utils import to_categorical #plot_model\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input, Dense, LSTM, CuDNNLSTM, Embedding, Dropout, Bidirectional\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2e2eCype9pii"
   },
   "source": [
    "# Helper functions for loading and preprocessing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pLJ16m2ixY9x"
   },
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lXB8EqOzxYOW"
   },
   "outputs": [],
   "source": [
    "def data_generator(descriptions, photos, tokenizer, max_length):\n",
    "    # loop for ever over images\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            # retrieve the photo feature\n",
    "            photo = photos[key][0]\n",
    "            in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo)\n",
    "            yield [[in_img, in_seq], out_word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ItIAKJX-31Q7"
   },
   "source": [
    "## Load some file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R7-Fi-hA3wJ8"
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    \n",
    "    # read all text\n",
    "    text = file.read()\n",
    "\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SKP0Z6Nzrta6"
   },
   "source": [
    "## Load descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "colab_type": "code",
    "id": "GzmOTITzphNs",
    "outputId": "c6575810-3998-46ef-8ac3-62c14d1f92ac"
   },
   "outputs": [],
   "source": [
    "# extract descriptions for images\n",
    "def load_descriptions(doc):\n",
    "    mapping = dict()\n",
    "    \n",
    "    # process lines\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "            \n",
    "        # take the first token as the image id, the rest as the description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        \n",
    "        # remove filename from image id\n",
    "        image_id = image_id.split('.')[0]\n",
    "        \n",
    "        # convert description tokens back to string\n",
    "        image_desc = ' '.join(image_desc)\n",
    "        \n",
    "        # create the list if needed\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = list()\n",
    "            \n",
    "        # store description\n",
    "        mapping[image_id].append(image_desc)\n",
    "        \n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K1mfSEvnqO3A"
   },
   "source": [
    "## Clean text in order to reduce the size of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "51Kjfu5EqEb8",
    "outputId": "2d2de0b9-47c5-4e75-e34b-1b680be209ac"
   },
   "outputs": [],
   "source": [
    "import string\n",
    " \n",
    "def clean_descriptions(descriptions):\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "    for key, desc_list in descriptions.items():\n",
    "        for i in range(len(desc_list)):\n",
    "            desc = desc_list[i]\n",
    "            # tokenize\n",
    "            desc = desc.split()\n",
    "            \n",
    "            # convert to lower case\n",
    "            desc = [word.lower() for word in desc]\n",
    "            \n",
    "            # remove punctuation from each token\n",
    "            desc = [w.translate(table) for w in desc]\n",
    "            \n",
    "            # remove hanging 's' and 'a'\n",
    "            desc = [word for word in desc if len(word)>1]\n",
    "            \n",
    "            # remove tokens with numbers in them\n",
    "            desc = [word for word in desc if word.isalpha()]\n",
    "            \n",
    "            # store as string\n",
    "            desc_list[i] =  ' '.join(desc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zRi45_Nbq076"
   },
   "source": [
    "## Convert descriptions to vocabulary and save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "colab_type": "code",
    "id": "sQZ6VwDlqri4",
    "outputId": "fe961440-58d8-42b7-c65b-fbb5abab73a5"
   },
   "outputs": [],
   "source": [
    "# convert the loaded descriptions into a vocabulary of words\n",
    "def to_vocabulary(descriptions):\n",
    "    # build a list of all description strings\n",
    "    all_desc = set()\n",
    "    \n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "        \n",
    "    return all_desc\n",
    "\n",
    "# save descriptions to file, one per line\n",
    "def save_descriptions(descriptions, filename):\n",
    "    lines = list()\n",
    "    \n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + ' ' + desc)\n",
    "            \n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cmTtkj0hsEnc"
   },
   "source": [
    "## Load a pre-defined set of identifiers given the sets filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p0lZdIwFsFTn"
   },
   "outputs": [],
   "source": [
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    \n",
    "    # process line by line\n",
    "    for line in doc.split('\\n'):\n",
    "        # skip empty lines\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "            \n",
    "        # get the image identifier\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "        \n",
    "    return set(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "elQuaHdnsYXh"
   },
   "source": [
    "## Load clean description into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6opUnibfsYni"
   },
   "outputs": [],
   "source": [
    "# load clean descriptions into memory, descriptions is a dictionary with image id as key and descriptions as values\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "    # load document\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    \n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        \n",
    "        # split id from description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        \n",
    "        # skip images not in the set\n",
    "        if image_id in dataset:\n",
    "            # create list\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "                \n",
    "            # wrap description in tokens\n",
    "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "            \n",
    "            # store\n",
    "            descriptions[image_id].append(desc)\n",
    "            \n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eLoJEWkAsv9I"
   },
   "source": [
    "## Loads the entire set of photo descriptions and returns a subset of interest for a given set of photo identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b2k6xLZOswGw"
   },
   "outputs": [],
   "source": [
    "# load photo features, connects features to photos\n",
    "def load_photo_features(filename, dataset):\n",
    "    # load all features\n",
    "    all_features = np.load(filename).item() # all_features = load(open(filename, 'rb'))\n",
    "    \n",
    "    # filter features\n",
    "    features = {k: all_features[k] for k in dataset}\n",
    "    \n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v3m0CZ9dtOpY"
   },
   "source": [
    "## Convert the dictionary of descriptions into a list of strings and l fit a Tokenizer given the loaded photo description text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "colab_type": "code",
    "id": "mBNScxh-tPQx",
    "outputId": "3b5f113c-50ce-4b8c-a073-d613071e0ca7"
   },
   "outputs": [],
   "source": [
    "# covert a dictionary of descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "        \n",
    "    return all_desc\n",
    " \n",
    "# fit a tokenizer given caption descriptions, creates a indexes for each unique word in descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X5ufI4nBtnaX"
   },
   "source": [
    "## Create sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WG89zvY7tnyn"
   },
   "outputs": [],
   "source": [
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    \n",
    "    # walk through each description for the image\n",
    "    for desc in desc_list:\n",
    "        # encode the sequence\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0] # text to list of indices defined by tokenizer\n",
    "        \n",
    "        # split one sequence into multiple X,Y pairs\n",
    "        for i in range(1, len(seq)):\n",
    "            # split into input and output pair\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            \n",
    "            # pad input sequence\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0] # pads sequence to length of max sequence\n",
    "            \n",
    "            # encode output sequence\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size_train)[0]\n",
    "            \n",
    "            # store\n",
    "            X1.append(photo)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "\n",
    "    return np.array(X1), np.array(X2), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eGfNU6ZPt9_m"
   },
   "source": [
    "## Define length of description with most words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ylGz95ipt-XQ"
   },
   "outputs": [],
   "source": [
    "# calculate the length of the description with the most words (also defines padding)\n",
    "def max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    max_val = max(len(d.split()) for d in lines) # Added variable to return due to error(?)\n",
    "    return max_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OiJVb_6D2kBl"
   },
   "source": [
    "## Extract features from VGG16 or InceptionResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0gr5aABE2kWo"
   },
   "outputs": [],
   "source": [
    "class WrongModelType(Exception):\n",
    "    pass\n",
    "\n",
    "# extract features from each photo in the directory\n",
    "def extract_features(directory, model_type):\n",
    "    # load the model    \n",
    "    if model_type.lower() == 'vgg16':\n",
    "        model = VGG16()\n",
    "    elif model_type.lower() == 'inceptionresnet':\n",
    "        model = InceptionResNetV2()\n",
    "    else:\n",
    "        raise WrongModelType('Pick a valid model_type! Either \"VGG16\" or \"InceptionResNet\"')\n",
    "    \n",
    "    # re-structure the model\n",
    "    model.layers.pop()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "\n",
    "    # summarize\n",
    "    print(model.summary())\n",
    "\n",
    "    # extract features from each photo\n",
    "    features = dict()\n",
    "    for name in listdir(directory):\n",
    "        # load an image from file\n",
    "        filename = directory + '/' + name\n",
    "        image = load_img(filename, target_size=(224, 224))\n",
    "        \n",
    "        # convert the image pixels to a numpy array\n",
    "        image = img_to_array(image)\n",
    "        \n",
    "        # reshape data for the model\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        \n",
    "        # prepare the image\n",
    "        if model_type.lower() is 'vgg16':\n",
    "            image = preprocess_VGG16(image)\n",
    "        elif model_type.lower() is 'inceptionresnet':\n",
    "            image = preprocess_InceptionResNet(image)\n",
    "        \n",
    "        # get features\n",
    "        feature = model.predict(image, verbose=0)\n",
    "        \n",
    "        # get image id\n",
    "        image_id = name.split('.')[0]\n",
    "        \n",
    "        # store feature\n",
    "        features[image_id] = feature\n",
    "        print('>%s' % name)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_single_features(filename, model_type):\n",
    "    # load the model   \n",
    "    if model_type.lower() == 'vgg16':\n",
    "        model = VGG16()\n",
    "    elif model_type.lower() == 'inceptionresnet':\n",
    "        model = InceptionResNetV2()\n",
    "    else:\n",
    "        raise WrongModelType('Pick a valid model_type! Either \"VGG16\" or \"InceptionResNet\"')\n",
    "        \n",
    "    # re-structure the model\n",
    "    model.layers.pop()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    \n",
    "    # load the photo\n",
    "    image = load_img(filename, target_size=(224, 224))\n",
    "    \n",
    "    # convert the image pixels to a numpy array\n",
    "    image = img_to_array(image)\n",
    "    \n",
    "    # reshape data for the model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    \n",
    "    # prepare the image\n",
    "    if model_type.lower() is 'vgg16':\n",
    "        image = preprocess_VGG16(image)\n",
    "    elif model_type.lower() is 'inceptionresnet':\n",
    "        image = preprocess_InceptionResNet(image)\n",
    "    \n",
    "    # get features\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZQI-eMj8HyJ"
   },
   "source": [
    "## Map integer to word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q5Qxvlga8IIA"
   },
   "outputs": [],
   "source": [
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i4EKXkID8IXd"
   },
   "source": [
    "## Generate image description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2aKYkZ2f8Id4"
   },
   "outputs": [],
   "source": [
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    # seed the generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the whole length of the sequence\n",
    "    for i in range(max_length):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        # convert probability to integer\n",
    "        yhat = np.argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JUPlaV28LwXh"
   },
   "source": [
    "## Evaluate model with BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8JNP08biL0Fz"
   },
   "outputs": [],
   "source": [
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = list(), list()\n",
    "    # step over the whole set\n",
    "    for key, desc_list in descriptions.items():\n",
    "        # generate description\n",
    "        yhat = generate_desc(model, tokenizer_train, photos[key], max_length)\n",
    "        \n",
    "        # store actual and predicted\n",
    "        references = [d.split() for d in desc_list]\n",
    "        actual.append(references)\n",
    "        predicted.append(yhat.split())\n",
    "\n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def BLEU(y_true, y_pred):\n",
    "    pred_list = []\n",
    "    true_list = []\n",
    "    for i, caption in enumerate(y_pred): # loop over all predictions (n_samples,)\n",
    "        pred_text = ''\n",
    "        for index in caption:\n",
    "            word = word_for_id(index, tokenizer) # convert the prediced word sequence(indices) to words using tokenizer \n",
    "            if word is None: # stop if we cannot map the word\n",
    "                break\n",
    "            pred_text += ' ' + word\n",
    "            if word == 'endseq': # stop if we predict the end of the sequence\n",
    "                break\n",
    "        pred_list[i] = pred_text[1:-7].split() # BLEU only cares for the predicted sentence converted to a list of words\n",
    "        \n",
    "    for i, captions in enumerate(y_true): # loop over y_true (nSamples, 5) where 5 is captions per sample image\n",
    "        true_captions = []\n",
    "        for j, caption in enumerate(captions): # each caption sequence to be converted to sentence\n",
    "            true_text = ''\n",
    "            for index in caption: # for each index in caption convert to word and add up\n",
    "                word = word_for_id(index, tokenizer)\n",
    "                if word is None: # stop if we cannot map the word\n",
    "                    break\n",
    "                true_text += ' ' + word\n",
    "                if word == 'endseq': # stop if we predict the end of the sequence\n",
    "                    break\n",
    "            true_captions[j] = true_text[1:-7].split()\n",
    "        true_list[i] = true_captions\n",
    "    \n",
    "    bleu1 = mean([corpus_bleu(true_list[i], pred_list[i], weights=(1.0, 0, 0, 0)) for i in len(pred_list)])\n",
    "    #bleu2 = mean([corpus_bleu(true_list[i], pred_list[i], weights=(0.5, 0.5, 0, 0)) for i in len(pred_list)])\n",
    "    #bleu3 = mean([corpus_bleu(true_list[i], pred_list[i], weights=(0.33, 0.33, 0.33, 0)) for i in len(pred_list)])\n",
    "    #bleu4 = mean([corpus_bleu(true_list[i], pred_list[i], weights=(0.25, 0.25, 0.25, 0.25)) for i in len(pred_list)])\n",
    "    \n",
    "            \n",
    "    return blue1 #, bleu2, bleu3, bleu4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations for defining models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 8092 \n",
      "Vocabulary Size: 8763\n"
     ]
    }
   ],
   "source": [
    "filename = 'Flickr8k_text/Flickr8k.token.txt'\n",
    "# load descriptions\n",
    "doc = load_doc(filename)\n",
    "# parse descriptions\n",
    "descriptions = load_descriptions(doc)\n",
    "print('Loaded: %d ' % len(descriptions))\n",
    "# clean descriptions\n",
    "clean_descriptions(descriptions)\n",
    "# summarize vocabulary\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Vocabulary Size: %d' % len(vocabulary))\n",
    "# save to file\n",
    "save_descriptions(descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset train: 6000\n",
      "Dataset val: 1000\n",
      "Descriptions: train=6000\n",
      "Descriptions: val=1000\n",
      "Photos: train=6000\n",
      "Photos: val=1000\n",
      "Vocabulary Size train: 7579\n",
      "Vocabulary Size val: 3317\n",
      "Description Length train: 34\n",
      "Description Length val: 31\n"
     ]
    }
   ],
   "source": [
    "# load training dataset (6K)\n",
    "filename_train = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "filename_val = 'Flickr8k_text/Flickr_8k.devImages.txt'\n",
    "train = load_set(filename_train)\n",
    "val = load_set(filename_val)\n",
    "print('Dataset train: %d' % len(train))\n",
    "print('Dataset val: %d' % len(val))\n",
    "\n",
    "# descriptions\n",
    "descriptions_train = load_clean_descriptions('descriptions.txt', train)\n",
    "descriptions_val = load_clean_descriptions('descriptions.txt', val)\n",
    "print('Descriptions: train=%d' % len(descriptions_train))\n",
    "print('Descriptions: val=%d' % len(descriptions_val))\n",
    "\n",
    "# photo features\n",
    "features_train = load_photo_features('features_VGG.npy', train)\n",
    "features_val = load_photo_features('features_VGG.npy', val)\n",
    "print('Photos: train=%d' % len(features_train))\n",
    "print('Photos: val=%d' % len(features_val))\n",
    "\n",
    "# prepare tokenizer\n",
    "tokenizer_train = create_tokenizer(descriptions_train)\n",
    "tokenizer_val = create_tokenizer(descriptions_val)\n",
    "np.save('tokenizer_train.npy',tokenizer_train)\n",
    "np.save('tokenizer_val.npy',tokenizer_val)\n",
    "vocab_size_train = len(tokenizer_train.word_index) + 1\n",
    "vocab_size_val = len(tokenizer_val.word_index) + 1\n",
    "print('Vocabulary Size train: %d' % vocab_size_train)\n",
    "print('Vocabulary Size val: %d' % vocab_size_val)\n",
    "\n",
    "# determine the maximum sequence length\n",
    "max_len_train = max_length(descriptions_train)\n",
    "max_len_val = max_length(descriptions_val)\n",
    "print('Description Length train: %d' % max_len_train)\n",
    "print('Description Length val: %d' % max_len_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard caption model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model_base(vocab_size, max_length):\n",
    "    # feature extractor model\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    \n",
    "    # sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256)(inputs2) #, mask_zero=True\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = CuDNNLSTM(256)(se2)\n",
    "    \n",
    "    # decoder model\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    \n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[BLEU])\n",
    "    \n",
    "    # summarize model\n",
    "    model.summary()\n",
    "    #plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified caption model (to be continued...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model with GLOVE embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.300d.txt'),encoding=\"utf-8\")\n",
    "iter = 0\n",
    "for line in f:\n",
    "    iter+=1\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300 #Since we use 'glove.6B.300d.txt'\n",
    "word_index = tokenizer_train.word_index \n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size_train, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(vocab_size_train,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_len_train,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model_glove(vocab_size, max_length):\n",
    "    # feature extractor model\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    \n",
    "    # sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = embedding_layer(inputs2) #, mask_zero=True\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = CuDNNLSTM(256)(se2)\n",
    "    \n",
    "    # decoder model\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    \n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    # summarize model\n",
    "    model.summary()\n",
    "    #plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model with bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model_bidirectional(vocab_size, max_length):\n",
    "    # feature extractor model\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    \n",
    "    # sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256)(inputs2) #, mask_zero=True\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = Bidirectional(CuDNNLSTM(256),merge_mode='ave')(se2)\n",
    "    se4 = Dense(256, activation='relu')(se3)\n",
    "    \n",
    "    # decoder model\n",
    "    de1 = add([fe2, se4])\n",
    "    de2 = Dense(256, activation='relu')(de1)\n",
    "    de3 = Bidirectional(CuDNNLSTM(256),merge_mode='ave')(de2)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(de3)\n",
    "    \n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[BLEU])\n",
    "    \n",
    "    # summarize model\n",
    "    model.summary()\n",
    "    #plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspiration:\n",
    "* [Image captioning in Keras](https://github.com/danieljl/keras-image-captioning)\n",
    "* [Keras implementation of image captioning model](https://medium.com/@faizanmustafa75/keras-implementation-of-image-captioning-model-3a7ab68e67d4)\n",
    "* [Image Captioning using InceptionV3 and beam search](https://github.com/yashk2810/Image-Captioning)\n",
    "* [Recurrent Neural Networks, Image Captioning, LSTM](https://www.youtube.com/watch?v=cO0a0QYmFm8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DgKlaU37-9JG"
   },
   "source": [
    "# Training and tweaking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 feature extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract features from all images\n",
    "directory = 'Flickr8k_Dataset'\n",
    "features = extract_features(directory,model_type='VGG16p')\n",
    "\n",
    "# save to file\n",
    "np.save('features_VGG16.npy',features)\n",
    "#imported_features = np.load('features.npy').item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InceptionResNet feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract features from all images\n",
    "directory = 'Flickr8k_Dataset'\n",
    "features = extract_features(directory,model_type='InceptionResNet')\n",
    "\n",
    "# save to file\n",
    "np.save('features_InceptionResNet.npy',features)\n",
    "#imported_features = np.load('features.npy').item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VbO6lZm7uUmB"
   },
   "source": [
    "## Base model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bLMr8wPs_Fwa"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Tensor objects are not iterable when eager execution is not enabled. To iterate over this tensor use tf.map_fn.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-c4116e6df66b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# define the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel_VGG16\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefine_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# train the model, run epochs manually and save after each epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-107-c4dd9716869d>\u001b[0m in \u001b[0;36mdefine_model\u001b[1;34m(vocab_size, max_length)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# compile model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mBLEU\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# summarize model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dmlgpu2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[0;32m    438\u001b[0m                 \u001b[0moutput_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnested_metrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m                 \u001b[0moutput_weighted_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnested_weighted_metrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                 \u001b[0mhandle_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_metrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m                 \u001b[0mhandle_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_weighted_metrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dmlgpu2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mhandle_metrics\u001b[1;34m(metrics, weights)\u001b[0m\n\u001b[0;32m    407\u001b[0m                     metric_result = weighted_metric_fn(y_true, y_pred,\n\u001b[0;32m    408\u001b[0m                                                        \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m                                                        mask=masks[i])\n\u001b[0m\u001b[0;32m    410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m                 \u001b[1;31m# Append to self.metrics_names, self.metric_tensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dmlgpu2\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mweighted\u001b[1;34m(y_true, y_pred, weights, mask)\u001b[0m\n\u001b[0;32m    401\u001b[0m         \"\"\"\n\u001b[0;32m    402\u001b[0m         \u001b[1;31m# score_array has ndim >= 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m         \u001b[0mscore_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    404\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m             \u001b[1;31m# Cast the mask to floatX to avoid float64 upcasting in Theano\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-106-fcdf94b6cc96>\u001b[0m in \u001b[0;36mBLEU\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mpred_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtrue_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaption\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# loop over all predictions (n_samples,)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mpred_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcaption\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dmlgpu2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m       raise TypeError(\n\u001b[1;32m--> 431\u001b[1;33m           \u001b[1;34m\"Tensor objects are not iterable when eager execution is not \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    432\u001b[0m           \"enabled. To iterate over this tensor use tf.map_fn.\")\n\u001b[0;32m    433\u001b[0m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Tensor objects are not iterable when eager execution is not enabled. To iterate over this tensor use tf.map_fn."
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model_base = define_model_base(vocab_size_train, max_len)\n",
    "\n",
    "# train the model, run epochs manually and save after each epoch\n",
    "num_epochs = 10\n",
    "steps_train = len(descriptions_train)\n",
    "steps_val = len(descriptions_val)\n",
    "\n",
    "# callbacks\n",
    "ES = EarlyStopping(monitor='val_loss') #, restore_best_weights=True\n",
    "\n",
    "# create the data generator\n",
    "generator_train = data_generator(descriptions_train, features_train, tokenizer_train, max_len_train)\n",
    "generator_val = data_generator(descriptions_val, features_val, tokenizer_train, max_len_train)\n",
    "# TODO: ADD VALIDATION GENERATOR\n",
    "\n",
    "# fit for one epoch\n",
    "model_base.fit_generator(generator_train, epochs=num_epochs, steps_per_epoch=steps_train, verbose=1, callbacks=[ES], validation_data=generator_val, validation_steps=steps_val)\n",
    "\n",
    "# save model\n",
    "model_base.save('model_base.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer bidirectional_12: expected ndim=3, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-131-19fab8367f32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# define the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel_mod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefine_model_bidirectional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# train the model, run epochs manually and save after each epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-130-4c30fa7ba644>\u001b[0m in \u001b[0;36mdefine_model_bidirectional\u001b[1;34m(vocab_size, max_length)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mde1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfe2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mse4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mde2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mde1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mde3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCuDNNLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmerge_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ave'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mde2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mde3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dmlgpu2\\lib\\site-packages\\keras\\layers\\wrappers.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m         \u001b[1;31m# Applies the same workaround as in `RNN.__call__`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dmlgpu2\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    412\u001b[0m                 \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m                 \u001b[1;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 414\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m                 \u001b[1;31m# Collect input shapes to build layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dmlgpu2\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m                                      str(K.ndim(x)))\n\u001b[0m\u001b[0;32m    312\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 is incompatible with layer bidirectional_12: expected ndim=3, found ndim=2"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model_mod = define_model_bidirectional(vocab_size_train, max_len_train)\n",
    "\n",
    "# train the model, run epochs manually and save after each epoch\n",
    "num_epochs = 10\n",
    "steps_train = len(descriptions_train)\n",
    "steps_val = len(descriptions_val)\n",
    "\n",
    "# callbacks\n",
    "ES = EarlyStopping(monitor='val_loss') #, restore_best_weights=True\n",
    "\n",
    "# create the data generator\n",
    "generator_train = data_generator(descriptions_train, features_train, tokenizer_train, max_len_train)\n",
    "generator_val = data_generator(descriptions_val, features_val, tokenizer_train, max_len_train)\n",
    "# TODO: ADD VALIDATION GENERATOR\n",
    "\n",
    "# fit for one epoch\n",
    "model_mod.fit_generator(generator_train, epochs=num_epochs, steps_per_epoch=steps_train, verbose=1, validation_data=generator_val, validation_steps=steps_val)\n",
    "#, callbacks=[ES]\n",
    "\n",
    "# save model\n",
    "model_mod.save('model_modified.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JQ5VMW66_XN4"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bxKirLjU0ma2"
   },
   "outputs": [],
   "source": [
    "# load test set\n",
    "filename = 'Flickr8k_text/Flickr_8k.testImages.txt'\n",
    "test = load_set(filename)\n",
    "print('Dataset: %d' % len(test))\n",
    "# descriptions\n",
    "test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
    "print('Descriptions: test=%d' % len(test_descriptions))\n",
    "# photo features\n",
    "test_features = load_photo_features('features_InceptionResNet.npy', test)\n",
    "print('Photos: test=%d' % len(test_features))\n",
    " \n",
    "# load the model\n",
    "filename = 'model_modified.h5'\n",
    "model = load_model(filename)\n",
    "\n",
    "# evaluate model\n",
    "evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Souv8KZ1M4i"
   },
   "source": [
    "## Generate caption for single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tOgF6Zvc1P6p"
   },
   "outputs": [],
   "source": [
    "# load the tokenizer\n",
    "tokenizer = np.load('tokenizer.npy').item()\n",
    "\n",
    "# pre-define the max sequence length (from training)\n",
    "max_length = 34\n",
    "\n",
    "# load the model\n",
    "model = load_model('model_3.h5')\n",
    "\n",
    "# load and prepare the photograph\n",
    "photo = extract_single_features('dog-frisbee.jpg','VGG16')\n",
    "\n",
    "# generate description\n",
    "description = generate_desc(model, tokenizer, photo, max_length)\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x_L7PFkA1uXO"
   },
   "source": [
    "# EXTRA: Possible extensions\n",
    "\n",
    "*  Pre-trained models: InceptionResNetV2 https://arxiv.org/pdf/1602.07261.pdf\n",
    "*  Pre-trained Word Vectors [Pre-trained word embeddings](https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py)\n",
    "*  Bidirectional - [Wrappers](https://keras.io/layers/wrappers/)\n",
    "*  Tune Model\n",
    "*  (Attention)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DML_Project.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
